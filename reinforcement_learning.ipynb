{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe326b93-e15a-4a6d-b81b-f9ee07332b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# # Lista os dispositivos físicos disponíveis para o TensorFlow\n",
    "# print(\"Dispositivos físicos disponíveis: \", tf.config.list_physical_devices())\n",
    "\n",
    "# # Define as GPUs como não disponíveis\n",
    "# tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# # Verifica se o TensorFlow ainda consegue ver a GPU\n",
    "# visible_devices = tf.config.get_visible_devices()\n",
    "# for device in visible_devices:\n",
    "#     assert device.device_type != 'GPU'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "546c66da-8f1a-4678-907c-223f24e6fb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\envs\\reinforcementlearning_tf1_p36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\anaconda3\\envs\\reinforcementlearning_tf1_p36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\anaconda3\\envs\\reinforcementlearning_tf1_p36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\anaconda3\\envs\\reinforcementlearning_tf1_p36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\anaconda3\\envs\\reinforcementlearning_tf1_p36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\anaconda3\\envs\\reinforcementlearning_tf1_p36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import imageio\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50a9f3f2-5c94-42ea-a372-17589aa0562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "LEN_MEMORY_QUEUE = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aac0b4e5-f68a-4337-a927-217a77c3007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def huber_loss(y_true, y_pred, clip_delta=1.0):\n",
    "    error = y_true - y_pred\n",
    "    cond  = tf.keras.backend.abs(error) < clip_delta\n",
    "\n",
    "    squared_loss = 0.5 * tf.keras.backend.square(error)\n",
    "    linear_loss  = clip_delta * (tf.keras.backend.abs(error) - 0.5 * clip_delta)\n",
    "\n",
    "    return tf.where(cond, squared_loss, linear_loss)\n",
    "\n",
    "def huber_loss_mean(y_true, y_pred, clip_delta=1.0):\n",
    "    return tf.keras.backend.mean(huber_loss(y_true, y_pred, clip_delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09ab3c61-85e5-43ac-b754-f72abe9b5df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(): \n",
    "    def __init__(self, state_size, action_size):\n",
    "        #self.weight_backup              =       experiment_params['dirs']['dir_model']+experiment_params['game']+\".h5\"\n",
    "        self.state_size                 =       state_size\n",
    "        self.action_size                =       action_size\n",
    "        self.memory                     =       deque(maxlen=LEN_MEMORY_QUEUE)\n",
    "        self.min_learning_rate          =       .001\n",
    "        self.max_learning_rate          =       .0008\n",
    "        self.epochs_interval_lr         =       1\n",
    "        self.learning_rate_decay        =       (self.max_learning_rate - self.min_learning_rate) / self.epochs_interval_lr\n",
    "        self.gamma                      =       .99\n",
    "        self.exploration_rate           =       .7\n",
    "        self.exploration_min            =       .1\n",
    "        self.exploration_decay          =       1 / 90000 # A variável irá se atualizar.\n",
    "#        self.exploration_map            =       experiment_params['params_agent']['exploration_map']\n",
    "        self.k_frames                   =       4\n",
    "        self.frame_height               =       self.state_size[0]\n",
    "        self.frame_width                =       self.state_size[1]\n",
    "        self.brain                      =       self._build_model()\n",
    "        self.brain_target               =       self._build_model()\n",
    "        self.freq_update_nn             =       1000\n",
    "        self.initial_start_size         =       5000\n",
    "        \n",
    "        #self.update_exploration_decay(0)\n",
    "\n",
    "    # def update_exploration_decay(self, frame):\n",
    "    #     for frame_limit, eps_limit in self.exploration_map:\n",
    "    #         if frame <= frame_limit:\n",
    "    #             self.exploration_decay =  (eps_limit[0] - eps_limit[1]) / (frame_limit/self.k_frames)\n",
    "    #             break\n",
    "        \n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        \n",
    "        ## Build Convolutional NetWork\n",
    "        model.add(Conv2D(32, (8,8), strides=4, input_shape=(self.state_size[0], self.state_size[1], self.k_frames), activation='relu', padding='valid'))\n",
    "        model.add(Conv2D(64, (4,4), strides=2, activation='relu', padding='valid'))\n",
    "        model.add(Conv2D(64, (3,3), strides=1, activation='relu', padding='valid'))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        ## Build Neural Network\n",
    "        model.add(Dense(512, activation='relu', kernel_initializer='he_uniform'))          \n",
    "        #model.add(Dense(256, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "        #model.add(Dense(64, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "            \n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss=huber_loss_mean, optimizer=Adam(lr=self.max_learning_rate))\n",
    "            \n",
    "        model.summary()\n",
    "        # if self.has_load_data():\n",
    "        #     model.load_weights(self.weight_backup)\n",
    "        #     self.exploration_rate = self.exploration_min\n",
    "        #     self.max_learning_rate = self.min_learning_rate\n",
    "        return model\n",
    "\n",
    "    \n",
    "    # def save_model(self):\n",
    "    #     self.brain.save(self.weight_backup)\n",
    "        \n",
    "            \n",
    "    def get_last_k_frames(self, state):\n",
    "        frames = np.empty((self.k_frames, self.frame_height, self.frame_width))\n",
    "\n",
    "        for i in range(0, self.k_frames):\n",
    "            _, _, _, n_s, _ = self.memory[len(self.memory)-(self.k_frames-i)]\n",
    "            frames[i] = n_s\n",
    "            \n",
    "        return np.transpose(frames, axes=(1,2,0))\n",
    "\n",
    "    \n",
    "    def act(self, state, current):\n",
    "        if np.random.rand() <= self.exploration_rate or len(self.memory) < self.k_frames+1 or current < self.initial_start_size:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        k_frames_state = self.get_last_k_frames(state)\n",
    "        k_frames_state = np.expand_dims(k_frames_state, axis=0)\n",
    "        #act_values = self.brain.predict(k_frames_state, verbose=0)\n",
    "        act_values = self.brain.predict(k_frames_state, verbose=0)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):        \n",
    "        self.memory.append((state, action, reward, next_state, done))      \n",
    "\n",
    "\n",
    "    def pack_K_frames(self, sample_batch_size):\n",
    "        if len(self.memory) < self.k_frames+2:\n",
    "            return\n",
    "        \n",
    "        state = np.empty((sample_batch_size, self.k_frames, self.frame_height, self.frame_width))\n",
    "        action = np.empty(sample_batch_size, dtype=np.uint8)\n",
    "        reward = np.empty(sample_batch_size, dtype=np.float32)\n",
    "        next_state = np.empty((sample_batch_size, self.k_frames, self.frame_height, self.frame_width))\n",
    "        done = np.empty(sample_batch_size, dtype=np.bool_)\n",
    "        \n",
    "\n",
    "        for k in range(sample_batch_size):\n",
    "            index = random.randint(0, len(self.memory)-self.k_frames-2)\n",
    "            \n",
    "            for i, idx_memory in enumerate(range(index, index+self.k_frames)):\n",
    "                s, a, r, n_s, d = self.memory[idx_memory]\n",
    "            \n",
    "                state[k][i] = s\n",
    "                next_state[k][i] = n_s\n",
    "                \n",
    "            done[k] = d\n",
    "            action[k] = a\n",
    "            reward[k] = r\n",
    "              \n",
    "        #State = (32,4,84,84)  -> State Transpose = (32,84,84,4) \n",
    "        return np.transpose(state, axes=(0,2,3,1)), action, reward, np.transpose(next_state, axes=(0,2,3,1)), done\n",
    "        \n",
    "\n",
    "    def replay(self, sample_batch_size):\n",
    "        if len(self.memory) < sample_batch_size:\n",
    "            return\n",
    "        #sample_batch = random.sample(self.memory, sample_batch_size)\n",
    "        state, action, reward, next_state, done = self.pack_K_frames(sample_batch_size)\n",
    "        \n",
    "        #print('State: {}'.format(state.shape))\n",
    "        #print('Action: {}'.format(action.shape))\n",
    "        #print('Reward: {}'.format(reward.shape))\n",
    "        #print('Next_State: {}'.format(next_state.shape))\n",
    "        #print('Done: {}'.format(done.shape))\n",
    "        #input()\n",
    "        #target = reward\n",
    "        predicted = self.brain_target.predict(next_state, verbose=0) #Previsão proximo estado.\n",
    "        target_f = self.brain.predict(state, verbose=0) #Previsão estado atual.\n",
    "        #print('Predicted: {}'.format(predicted))\n",
    "        #print('Predicted[0]: {}'.format(predicted[2]))\n",
    "        #print('Predicted Max: {}'.format(np.amax(predicted)))\n",
    "        #print('Reward: {}'.format(reward))\n",
    "        #print('Target_f: {}'.format(target_f))\n",
    "        #input()\n",
    "        \n",
    "        # for i in range(sample_batch_size):\n",
    "        #     target = reward[i] + (self.gamma * np.amax(predicted[i]) * (1-done[i]))\n",
    "        #     target_f[i][action[i]] = target\n",
    "        targets = reward + (self.gamma * np.amax(predicted, axis=1) * (1 - done))\n",
    "        target_f[np.arange(sample_batch_size), action] = targets\n",
    "   \n",
    "            \n",
    "        \n",
    "        #print('Target_f Formatado: {}'.format(target_f))\n",
    "        #input()\n",
    "        history = self.brain.fit(state, target_f, batch_size=sample_batch_size, epochs=1, verbose=0)\n",
    "            \n",
    "        if self.exploration_rate > self.exploration_min:\n",
    "            self.exploration_rate -= self.exploration_decay\n",
    "        \n",
    "        return history, self.exploration_rate\n",
    "\n",
    "    def update_target_model(self, current_frame):\n",
    "        if (current_frame % self.freq_update_nn == 0 and TRAIN and current_frame > self.initial_start_size):\n",
    "            self.brain_target.set_weights(self.brain.get_weights())\n",
    "            print('-------------------------UPDATED TARGET MODEL() ------------------------------')\n",
    "\n",
    "    def update_learning_rate(self,):\n",
    "        ## Modified \n",
    "        if self.max_learning_rate > self.min_learning_rate:\n",
    "            if self.epochs_interval_lr > 0:\n",
    "                self.max_learning_rate-=self.learning_rate_decay\n",
    "                self.epochs_interval_lr-=1\n",
    "                K.set_value(self.brain.optimizer.lr, self.max_learning_rate) # Change learning rate.\n",
    "                print('Current Learning rate: {}'.format(K.eval(self.brain.optimizer.lr)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e91f0054-4e0f-495a-9e22-f957c3ebda68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameConstructor():\n",
    "\n",
    "    def __init__(self, game):\n",
    "        self.env = gym.make(game)\n",
    "        self.sample_batch_size          =              32\n",
    "        self.episodes                   =              1000\n",
    "        self.action_size                =              self.env.action_space.n\n",
    "        self.state_size                 =              (84,84)\n",
    "        self.agent                      =              Agent(self.state_size, self.action_size)\n",
    "        self.best_score                 =              -99999999\n",
    "        self.crop_on_top                =              34\n",
    "        self.crop_on_bottom             =              16\n",
    "        self.crop_on_left               =              7\n",
    "        self.crop_on_right              =              7\n",
    "        self.frames_skip                =              1\n",
    "        self.current_frame              =              0\n",
    "        #self.freq_update                =              10000\n",
    "        #self.canSave                    =              True\n",
    "        self.time_train_init            =              time.time()\n",
    "#        self.epochs_to_save_results     =              experiment_params['epochs_to_save_results']\n",
    "        self.frames_in_atual_episode    =              0\n",
    "        self.seconds_in_atual_episode   =              0\n",
    "        self.freq_save_video            =              10\n",
    "\n",
    "        self.initialize_dirs()\n",
    "\n",
    "    def initialize_dirs(self):\n",
    "        for values in [\n",
    "            'experiments/Pong-useToPaper01/movies/'\n",
    "        ]:\n",
    "            if not os.path.isdir(values):\n",
    "                os.makedirs(values)\n",
    "    \n",
    "    def to_gray_scale(self, img):\n",
    "        return 0.299*img[:,:,0] + 0.587*img[:,:,1] + 0.114*img[:,:,2]\n",
    "    \n",
    "    def get_frames_per_seconds_in_atual_episode(self):\n",
    "        return int(self.frames_in_atual_episode / (time.time() - self.seconds_in_atual_episode))\n",
    "    \n",
    "   \n",
    "    def crop_img(self, img):\n",
    "        #return img[self.crop_on_top: -self.crop_on_bottom, self.crop_on_border:-self.crop_on_border]\n",
    "        return img[self.crop_on_top:-self.crop_on_bottom, self.crop_on_left:-self.crop_on_right]    \n",
    "\n",
    "    def preprocess_img(self, img):\n",
    "        return self.to_gray_scale(cv2.resize(self.crop_img(img), (self.state_size[0],self.state_size[0]), interpolation=cv2.INTER_AREA)) / 255\n",
    "        \n",
    "    \n",
    "    def save_image_epoch(self, gif_to_save, epoch, best_current_play=False):\n",
    "        if best_current_play:\n",
    "            epoch = str(epoch)+'_best'\n",
    "        else:\n",
    "            epoch = str(epoch)\n",
    "        \n",
    "        #if (self.canSave and len(gif_to_save) > 0):\n",
    "        if (len(gif_to_save) > 0):\n",
    "            dir_save = 'experiments/Pong-useToPaper01/movies/'\n",
    "            imageio.mimwrite(dir_save+epoch+'.mp4', np.multiply(gif_to_save, 255).astype(np.uint8), fps = 100)\n",
    "            print('Gif Salvo') \n",
    "            #self.canSave = False\n",
    "            \n",
    "    def restart_chronometer(self):\n",
    "        self.frames_in_atual_episode=0\n",
    "        self.seconds_in_atual_episode=0\n",
    "    \n",
    "    def run(self):\n",
    "        #global total_reward_game\n",
    "\n",
    "        try:\n",
    "            for i_episodes in range(self.episodes):\n",
    "                state = self.env.reset()\n",
    "               \n",
    "                state = self.preprocess_img(state)\n",
    "                \n",
    "                done = False\n",
    "                current_images_episode = [] \n",
    "                total_reward=0\n",
    "                history_list = []\n",
    "                exploration = 1.0\n",
    "                self.seconds_in_atual_episode= time.time()\n",
    "                \n",
    "                while not done:\n",
    "                                        \n",
    "                    #if i_episodes > 600 or not TRAIN:\n",
    "                    #self.env.render()\n",
    "                    action = self.agent.act(state, self.current_frame)\n",
    "                    next_state, reward, done, info = self.env.step(action)\n",
    "                    \n",
    "                    reward = np.sign(reward)\n",
    "                    total_reward+=reward\n",
    "                    next_state = self.preprocess_img(next_state)\n",
    "                    \n",
    "                    current_images_episode.append(next_state)\n",
    "\n",
    "                    \n",
    "                    self.agent.remember(state, action, reward, next_state, done)\n",
    "                    state = next_state\n",
    "                        \n",
    "                    self.current_frame+=1\n",
    "                    self.frames_in_atual_episode+=1\n",
    "                    if TRAIN:\n",
    "                        if self.current_frame > self.agent.initial_start_size and (self.current_frame % self.frames_skip) == 0:\n",
    "                           \n",
    "                            history, exploration = self.agent.replay(self.sample_batch_size)\n",
    "                            history_list.append(history.history['loss'])\n",
    "                    \n",
    "                        self.agent.update_target_model(self.current_frame)\n",
    "    \n",
    "                #total_reward_game.append(total_reward)\n",
    "                \n",
    "                \n",
    "                if TRAIN and self.current_frame > self.agent.initial_start_size:\n",
    "                    self.agent.update_learning_rate()\n",
    "                    #self.agent.update_exploration_decay(self.current_frame)\n",
    "                    if i_episodes % self.freq_save_video == 0:\n",
    "                        #self.canSave=True\n",
    "                        \n",
    "                        self.save_image_epoch( current_images_episode, i_episodes)   \n",
    "                    \n",
    "                    # self.add_new_result(total_reward, (self.current_frame / self.frames_skip), time.time(), \n",
    "                    #                     np.mean(history_list), exploration, self.get_frames_per_seconds_in_atual_episode(), K.eval(breakout.agent.brain.optimizer.lr))\n",
    "                   \n",
    "                    # if total_reward > self.best_score and self.current_frame > self.agent.initial_start_size:\n",
    "                    #         self.best_score = total_reward\n",
    "                    #         self.agent.save_model()\n",
    "                    #         self.save_image_epoch(current_images_episode, i_episodes, best_current_play=True)\n",
    "                    #         print('Save best current model -> ', end='')\n",
    "                current_images_episode = []\n",
    "                print(\"Episode {}# r: {}# Loss: {:.6} # Trains: {} # eps: {:.3}# Space: {}% 'fps: {}:\".format(i_episodes, total_reward, np.mean(history_list), (self.current_frame / self.frames_skip), exploration, round(((len(self.agent.memory) / (LEN_MEMORY_QUEUE)) * 100), 2), self.get_frames_per_seconds_in_atual_episode() ))\n",
    "                self.restart_chronometer()\n",
    "                \n",
    "                \n",
    "        finally:\n",
    "            if TRAIN:\n",
    "                print('Finish')\n",
    "                #self.agent.save_model()\n",
    "                #self.save_results()\n",
    "                \n",
    "                \n",
    "            self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c105c47-5df7-4b5d-a031-e305ca30a00e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\anaconda3\\envs\\reinforcementlearning_tf1_p36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 19, 19, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 6, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 1,261,222\n",
      "Trainable params: 1,261,222\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 19, 19, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 1,261,222\n",
      "Trainable params: 1,261,222\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mateus F\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\Mateus F\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0# r: -20.0# Loss: nan # Trains: 298.25 # eps: 1.0# Space: 2.39% 'fps: 1197:\n",
      "Episode 1# r: -20.0# Loss: nan # Trains: 600.0 # eps: 1.0# Space: 4.8% 'fps: 1270:\n",
      "Episode 2# r: -21.0# Loss: nan # Trains: 869.75 # eps: 1.0# Space: 6.96% 'fps: 1325:\n",
      "Episode 3# r: -20.0# Loss: nan # Trains: 1151.25 # eps: 1.0# Space: 9.21% 'fps: 1421:\n",
      "Episode 4# r: -18.0# Loss: nan # Trains: 1504.0 # eps: 1.0# Space: 12.03% 'fps: 1454:\n",
      "Episode 5# r: -20.0# Loss: nan # Trains: 1799.0 # eps: 1.0# Space: 14.39% 'fps: 1347:\n",
      "Episode 6# r: -21.0# Loss: nan # Trains: 2126.75 # eps: 1.0# Space: 17.01% 'fps: 1168:\n",
      "Episode 7# r: -21.0# Loss: nan # Trains: 2426.75 # eps: 1.0# Space: 19.41% 'fps: 926:\n",
      "Episode 8# r: -18.0# Loss: nan # Trains: 2800.0 # eps: 1.0# Space: 22.4% 'fps: 1093:\n",
      "Episode 9# r: -20.0# Loss: nan # Trains: 3092.75 # eps: 1.0# Space: 24.74% 'fps: 1414:\n",
      "Episode 10# r: -20.0# Loss: nan # Trains: 3420.75 # eps: 1.0# Space: 27.37% 'fps: 1322:\n",
      "Episode 11# r: -21.0# Loss: nan # Trains: 3713.5 # eps: 1.0# Space: 29.71% 'fps: 1431:\n",
      "Episode 12# r: -21.0# Loss: nan # Trains: 3994.5 # eps: 1.0# Space: 31.96% 'fps: 1454:\n",
      "Episode 13# r: -20.0# Loss: nan # Trains: 4337.25 # eps: 1.0# Space: 34.7% 'fps: 1432:\n",
      "Episode 14# r: -20.0# Loss: nan # Trains: 4656.0 # eps: 1.0# Space: 37.25% 'fps: 1439:\n",
      "Episode 15# r: -21.0# Loss: nan # Trains: 4917.5 # eps: 1.0# Space: 39.34% 'fps: 1430:\n",
      "Episode 16# r: -20.0# Loss: 0.00125899 # Trains: 5231.75 # eps: 0.697# Space: 41.85% 'fps: 48:\n",
      "Episode 17# r: -21.0# Loss: 0.00128573 # Trains: 5485.5 # eps: 0.695# Space: 43.88% 'fps: 32:\n",
      "Episode 18# r: -20.0# Loss: 0.00131377 # Trains: 5782.75 # eps: 0.691# Space: 46.26% 'fps: 37:\n",
      "Episode 19# r: -21.0# Loss: 0.00146158 # Trains: 6068.25 # eps: 0.688# Space: 48.55% 'fps: 35:\n",
      "Gif Salvo\n",
      "Episode 20# r: -20.0# Loss: 0.00137096 # Trains: 6365.25 # eps: 0.685# Space: 50.92% 'fps: 36:\n",
      "Episode 21# r: -21.0# Loss: 0.00133786 # Trains: 6635.0 # eps: 0.682# Space: 53.08% 'fps: 39:\n",
      "Episode 22# r: -21.0# Loss: 0.00148814 # Trains: 6898.25 # eps: 0.679# Space: 55.19% 'fps: 32:\n",
      "Episode 23# r: -21.0# Loss: 0.00133334 # Trains: 7196.0 # eps: 0.676# Space: 57.57% 'fps: 34:\n",
      "-------------------------UPDATED TARGET MODEL() ------------------------------\n",
      "Episode 24# r: -21.0# Loss: 0.00127092 # Trains: 7531.25 # eps: 0.672# Space: 60.25% 'fps: 36:\n",
      "Episode 25# r: -20.0# Loss: 0.00173636 # Trains: 7860.5 # eps: 0.668# Space: 62.88% 'fps: 36:\n",
      "Episode 26# r: -20.0# Loss: 0.00151177 # Trains: 8140.0 # eps: 0.665# Space: 65.12% 'fps: 33:\n",
      "Episode 27# r: -19.0# Loss: 0.00150091 # Trains: 8458.75 # eps: 0.662# Space: 67.67% 'fps: 36:\n",
      "Episode 28# r: -20.0# Loss: 0.00125022 # Trains: 8792.75 # eps: 0.658# Space: 70.34% 'fps: 39:\n",
      "Episode 29# r: -21.0# Loss: 0.00124739 # Trains: 9046.0 # eps: 0.655# Space: 72.37% 'fps: 34:\n",
      "Gif Salvo\n",
      "Episode 30# r: -21.0# Loss: 0.00152 # Trains: 9330.25 # eps: 0.652# Space: 74.64% 'fps: 38:\n",
      "Episode 31# r: -20.0# Loss: 0.00145802 # Trains: 9610.5 # eps: 0.649# Space: 76.88% 'fps: 36:\n",
      "Episode 32# r: -21.0# Loss: 0.00153271 # Trains: 9948.0 # eps: 0.645# Space: 79.58% 'fps: 39:\n",
      "-------------------------UPDATED TARGET MODEL() ------------------------------\n",
      "Episode 33# r: -21.0# Loss: 0.00128088 # Trains: 10252.0 # eps: 0.642# Space: 82.02% 'fps: 40:\n",
      "Episode 34# r: -20.0# Loss: 0.00138237 # Trains: 10531.75 # eps: 0.639# Space: 84.25% 'fps: 34:\n",
      "Episode 35# r: -20.0# Loss: 0.00151176 # Trains: 10823.5 # eps: 0.635# Space: 86.59% 'fps: 37:\n",
      "Episode 36# r: -21.0# Loss: 0.00162344 # Trains: 11109.5 # eps: 0.632# Space: 88.88% 'fps: 34:\n",
      "Episode 37# r: -21.0# Loss: 0.00135936 # Trains: 11368.25 # eps: 0.629# Space: 90.95% 'fps: 39:\n",
      "Episode 38# r: -21.0# Loss: 0.00145676 # Trains: 11678.5 # eps: 0.626# Space: 93.43% 'fps: 37:\n",
      "Episode 39# r: -21.0# Loss: 0.00129209 # Trains: 11931.25 # eps: 0.623# Space: 95.45% 'fps: 36:\n",
      "Gif Salvo\n",
      "Episode 40# r: -21.0# Loss: 0.00141403 # Trains: 12185.0 # eps: 0.62# Space: 97.48% 'fps: 35:\n",
      "Episode 41# r: -21.0# Loss: 0.0012841 # Trains: 12460.5 # eps: 0.617# Space: 99.68% 'fps: 39:\n",
      "-------------------------UPDATED TARGET MODEL() ------------------------------\n",
      "Episode 42# r: -20.0# Loss: 0.00135738 # Trains: 12746.25 # eps: 0.614# Space: 100.0% 'fps: 35:\n",
      "Episode 43# r: -21.0# Loss: 0.00144328 # Trains: 13013.5 # eps: 0.611# Space: 100.0% 'fps: 37:\n",
      "Episode 44# r: -21.0# Loss: 0.0014671 # Trains: 13279.75 # eps: 0.608# Space: 100.0% 'fps: 38:\n",
      "Episode 45# r: -21.0# Loss: 0.0014332 # Trains: 13535.5 # eps: 0.605# Space: 100.0% 'fps: 35:\n",
      "Episode 46# r: -20.0# Loss: 0.00127032 # Trains: 13839.75 # eps: 0.602# Space: 100.0% 'fps: 34:\n",
      "Episode 47# r: -19.0# Loss: 0.00146421 # Trains: 14165.25 # eps: 0.598# Space: 100.0% 'fps: 37:\n",
      "Episode 48# r: -21.0# Loss: 0.00143338 # Trains: 14437.75 # eps: 0.595# Space: 100.0% 'fps: 37:\n",
      "Episode 49# r: -21.0# Loss: 0.0014492 # Trains: 14717.5 # eps: 0.592# Space: 100.0% 'fps: 33:\n",
      "Gif Salvo\n",
      "Episode 50# r: -21.0# Loss: 0.0015818 # Trains: 14991.25 # eps: 0.589# Space: 100.0% 'fps: 36:\n",
      "-------------------------UPDATED TARGET MODEL() ------------------------------\n",
      "Episode 51# r: -20.0# Loss: 0.00136573 # Trains: 15270.25 # eps: 0.586# Space: 100.0% 'fps: 34:\n",
      "Episode 52# r: -19.0# Loss: 0.00126525 # Trains: 15638.5 # eps: 0.582# Space: 100.0% 'fps: 36:\n",
      "Episode 53# r: -20.0# Loss: 0.00141383 # Trains: 15919.5 # eps: 0.579# Space: 100.0% 'fps: 36:\n",
      "Episode 54# r: -21.0# Loss: 0.00154292 # Trains: 16190.0 # eps: 0.576# Space: 100.0% 'fps: 37:\n",
      "Episode 55# r: -20.0# Loss: 0.00123207 # Trains: 16497.75 # eps: 0.572# Space: 100.0% 'fps: 38:\n",
      "Episode 56# r: -21.0# Loss: 0.00158906 # Trains: 16771.5 # eps: 0.569# Space: 100.0% 'fps: 37:\n",
      "Episode 57# r: -20.0# Loss: 0.00144918 # Trains: 17088.5 # eps: 0.566# Space: 100.0% 'fps: 35:\n",
      "Episode 58# r: -21.0# Loss: 0.0014052 # Trains: 17353.75 # eps: 0.563# Space: 100.0% 'fps: 36:\n",
      "-------------------------UPDATED TARGET MODEL() ------------------------------\n",
      "Episode 59# r: -21.0# Loss: 0.00143529 # Trains: 17618.5 # eps: 0.56# Space: 100.0% 'fps: 36:\n",
      "Gif Salvo\n",
      "Episode 60# r: -20.0# Loss: 0.00139796 # Trains: 17954.25 # eps: 0.556# Space: 100.0% 'fps: 35:\n",
      "Episode 61# r: -20.0# Loss: 0.0016065 # Trains: 18233.75 # eps: 0.553# Space: 100.0% 'fps: 38:\n",
      "Episode 62# r: -20.0# Loss: 0.00129211 # Trains: 18516.75 # eps: 0.55# Space: 100.0% 'fps: 38:\n",
      "Episode 63# r: -21.0# Loss: 0.00165494 # Trains: 18770.0 # eps: 0.547# Space: 100.0% 'fps: 38:\n",
      "Episode 64# r: -21.0# Loss: 0.00138155 # Trains: 19051.5 # eps: 0.544# Space: 100.0% 'fps: 34:\n",
      "Episode 65# r: -21.0# Loss: 0.00149526 # Trains: 19327.0 # eps: 0.541# Space: 100.0% 'fps: 34:\n",
      "Episode 66# r: -20.0# Loss: 0.00135671 # Trains: 19630.5 # eps: 0.537# Space: 100.0% 'fps: 36:\n",
      "Episode 67# r: -21.0# Loss: 0.00139044 # Trains: 19896.5 # eps: 0.534# Space: 100.0% 'fps: 36:\n",
      "-------------------------UPDATED TARGET MODEL() ------------------------------\n",
      "Episode 68# r: -20.0# Loss: 0.0014515 # Trains: 20227.75 # eps: 0.531# Space: 100.0% 'fps: 34:\n",
      "Episode 69# r: -20.0# Loss: 0.00140785 # Trains: 20530.0 # eps: 0.527# Space: 100.0% 'fps: 38:\n",
      "Gif Salvo\n",
      "Episode 70# r: -21.0# Loss: 0.00162896 # Trains: 20810.25 # eps: 0.524# Space: 100.0% 'fps: 38:\n",
      "Episode 71# r: -21.0# Loss: 0.0012665 # Trains: 21117.0 # eps: 0.521# Space: 100.0% 'fps: 35:\n",
      "Episode 72# r: -20.0# Loss: 0.00130918 # Trains: 21441.25 # eps: 0.517# Space: 100.0% 'fps: 32:\n",
      "Episode 73# r: -21.0# Loss: 0.00139633 # Trains: 21693.75 # eps: 0.515# Space: 100.0% 'fps: 31:\n",
      "Episode 74# r: -19.0# Loss: 0.00137513 # Trains: 22003.75 # eps: 0.511# Space: 100.0% 'fps: 30:\n",
      "Episode 75# r: -19.0# Loss: 0.00139763 # Trains: 22314.25 # eps: 0.508# Space: 100.0% 'fps: 38:\n",
      "-------------------------UPDATED TARGET MODEL() ------------------------------\n",
      "Episode 76# r: -19.0# Loss: 0.00144614 # Trains: 22643.5 # eps: 0.504# Space: 100.0% 'fps: 28:\n",
      "Episode 77# r: -21.0# Loss: 0.00141077 # Trains: 22931.0 # eps: 0.501# Space: 100.0% 'fps: 27:\n",
      "Episode 78# r: -21.0# Loss: 0.00166291 # Trains: 23195.0 # eps: 0.498# Space: 100.0% 'fps: 28:\n",
      "Episode 79# r: -21.0# Loss: 0.00145467 # Trains: 23453.75 # eps: 0.495# Space: 100.0% 'fps: 31:\n",
      "Gif Salvo\n",
      "Episode 80# r: -21.0# Loss: 0.0015481 # Trains: 23748.5 # eps: 0.492# Space: 100.0% 'fps: 34:\n",
      "Episode 81# r: -21.0# Loss: 0.00154121 # Trains: 24011.0 # eps: 0.489# Space: 100.0% 'fps: 34:\n",
      "Episode 82# r: -21.0# Loss: 0.00146972 # Trains: 24315.25 # eps: 0.485# Space: 100.0% 'fps: 31:\n",
      "Episode 83# r: -21.0# Loss: 0.0013928 # Trains: 24568.0 # eps: 0.483# Space: 100.0% 'fps: 28:\n",
      "Episode 84# r: -21.0# Loss: 0.00140879 # Trains: 24843.75 # eps: 0.48# Space: 100.0% 'fps: 33:\n",
      "-------------------------UPDATED TARGET MODEL() ------------------------------\n",
      "Episode 85# r: -21.0# Loss: 0.00149417 # Trains: 25108.75 # eps: 0.477# Space: 100.0% 'fps: 36:\n",
      "Episode 86# r: -21.0# Loss: 0.001515 # Trains: 25384.25 # eps: 0.474# Space: 100.0% 'fps: 35:\n",
      "Episode 87# r: -20.0# Loss: 0.00155529 # Trains: 25722.75 # eps: 0.47# Space: 100.0% 'fps: 35:\n",
      "Episode 88# r: -20.0# Loss: 0.00145409 # Trains: 26004.5 # eps: 0.467# Space: 100.0% 'fps: 36:\n",
      "Episode 89# r: -21.0# Loss: 0.0013295 # Trains: 26269.5 # eps: 0.464# Space: 100.0% 'fps: 37:\n",
      "Gif Salvo\n",
      "Episode 90# r: -20.0# Loss: 0.00147641 # Trains: 26561.25 # eps: 0.46# Space: 100.0% 'fps: 38:\n",
      "Episode 91# r: -21.0# Loss: 0.00156355 # Trains: 26838.0 # eps: 0.457# Space: 100.0% 'fps: 39:\n",
      "Episode 92# r: -21.0# Loss: 0.00136663 # Trains: 27131.75 # eps: 0.454# Space: 100.0% 'fps: 35:\n",
      "Episode 93# r: -20.0# Loss: 0.00133948 # Trains: 27413.75 # eps: 0.451# Space: 100.0% 'fps: 35:\n",
      "-------------------------UPDATED TARGET MODEL() ------------------------------\n",
      "Episode 94# r: -20.0# Loss: 0.00138005 # Trains: 27709.75 # eps: 0.448# Space: 100.0% 'fps: 34:\n",
      "Episode 95# r: -19.0# Loss: 0.00155722 # Trains: 28037.0 # eps: 0.444# Space: 100.0% 'fps: 30:\n",
      "Episode 96# r: -19.0# Loss: 0.00145431 # Trains: 28353.75 # eps: 0.441# Space: 100.0% 'fps: 33:\n",
      "Episode 97# r: -20.0# Loss: 0.00146761 # Trains: 28643.5 # eps: 0.437# Space: 100.0% 'fps: 36:\n",
      "Episode 98# r: -21.0# Loss: 0.00145169 # Trains: 28981.75 # eps: 0.434# Space: 100.0% 'fps: 33:\n",
      "Episode 99# r: -21.0# Loss: 0.00149651 # Trains: 29257.25 # eps: 0.43# Space: 100.0% 'fps: 31:\n",
      "Gif Salvo\n",
      "Episode 100# r: -21.0# Loss: 0.00163507 # Trains: 29542.0 # eps: 0.427# Space: 100.0% 'fps: 28:\n",
      "Episode 101# r: -21.0# Loss: 0.00125101 # Trains: 29828.75 # eps: 0.424# Space: 100.0% 'fps: 31:\n",
      "-------------------------UPDATED TARGET MODEL() ------------------------------\n",
      "Episode 102# r: -20.0# Loss: 0.00135431 # Trains: 30111.0 # eps: 0.421# Space: 100.0% 'fps: 31:\n",
      "Episode 103# r: -20.0# Loss: 0.00159052 # Trains: 30406.25 # eps: 0.418# Space: 100.0% 'fps: 37:\n",
      "Episode 104# r: -21.0# Loss: 0.00127816 # Trains: 30686.25 # eps: 0.415# Space: 100.0% 'fps: 38:\n",
      "Episode 105# r: -21.0# Loss: 0.00143951 # Trains: 30941.75 # eps: 0.412# Space: 100.0% 'fps: 37:\n",
      "Episode 106# r: -21.0# Loss: 0.00155386 # Trains: 31258.0 # eps: 0.408# Space: 100.0% 'fps: 36:\n",
      "Episode 107# r: -20.0# Loss: 0.00141434 # Trains: 31563.5 # eps: 0.405# Space: 100.0% 'fps: 35:\n",
      "Episode 108# r: -20.0# Loss: 0.00154614 # Trains: 31891.75 # eps: 0.401# Space: 100.0% 'fps: 35:\n",
      "Episode 109# r: -21.0# Loss: 0.00151002 # Trains: 32202.5 # eps: 0.398# Space: 100.0% 'fps: 35:\n",
      "Gif Salvo\n",
      "Episode 110# r: -21.0# Loss: 0.00148652 # Trains: 32473.75 # eps: 0.395# Space: 100.0% 'fps: 36:\n",
      "-------------------------UPDATED TARGET MODEL() ------------------------------\n",
      "Episode 111# r: -21.0# Loss: 0.00150982 # Trains: 32765.75 # eps: 0.392# Space: 100.0% 'fps: 35:\n",
      "Episode 112# r: -20.0# Loss: 0.00143085 # Trains: 33088.75 # eps: 0.388# Space: 100.0% 'fps: 36:\n",
      "Episode 113# r: -20.0# Loss: 0.00120815 # Trains: 33378.0 # eps: 0.385# Space: 100.0% 'fps: 33:\n",
      "Episode 114# r: -20.0# Loss: 0.00139051 # Trains: 33666.75 # eps: 0.381# Space: 100.0% 'fps: 35:\n",
      "Episode 115# r: -21.0# Loss: 0.0014085 # Trains: 34027.75 # eps: 0.377# Space: 100.0% 'fps: 35:\n",
      "Episode 116# r: -21.0# Loss: 0.00136835 # Trains: 34281.0 # eps: 0.375# Space: 100.0% 'fps: 37:\n",
      "Episode 117# r: -21.0# Loss: 0.0013433 # Trains: 34564.75 # eps: 0.372# Space: 100.0% 'fps: 34:\n",
      "Episode 118# r: -21.0# Loss: 0.00158026 # Trains: 34849.75 # eps: 0.368# Space: 100.0% 'fps: 30:\n",
      "-------------------------UPDATED TARGET MODEL() ------------------------------\n",
      "Episode 119# r: -21.0# Loss: 0.00125549 # Trains: 35130.25 # eps: 0.365# Space: 100.0% 'fps: 36:\n",
      "Gif Salvo\n",
      "Episode 120# r: -20.0# Loss: 0.00140821 # Trains: 35431.0 # eps: 0.362# Space: 100.0% 'fps: 37:\n",
      "Episode 121# r: -21.0# Loss: 0.00150772 # Trains: 35712.0 # eps: 0.359# Space: 100.0% 'fps: 34:\n",
      "Episode 122# r: -21.0# Loss: 0.00143308 # Trains: 35986.0 # eps: 0.356# Space: 100.0% 'fps: 31:\n",
      "Episode 123# r: -20.0# Loss: 0.00132488 # Trains: 36267.0 # eps: 0.353# Space: 100.0% 'fps: 36:\n",
      "Episode 124# r: -21.0# Loss: 0.00154626 # Trains: 36550.5 # eps: 0.349# Space: 100.0% 'fps: 36:\n",
      "Episode 125# r: -21.0# Loss: 0.00143922 # Trains: 36823.75 # eps: 0.346# Space: 100.0% 'fps: 37:\n",
      "Episode 126# r: -21.0# Loss: 0.0015367 # Trains: 37097.0 # eps: 0.343# Space: 100.0% 'fps: 34:\n",
      "Episode 127# r: -21.0# Loss: 0.00147675 # Trains: 37361.25 # eps: 0.34# Space: 100.0% 'fps: 37:\n",
      "-------------------------UPDATED TARGET MODEL() ------------------------------\n",
      "Episode 128# r: -20.0# Loss: 0.00137315 # Trains: 37651.0 # eps: 0.337# Space: 100.0% 'fps: 38:\n",
      "Episode 129# r: -21.0# Loss: 0.00134568 # Trains: 37920.75 # eps: 0.334# Space: 100.0% 'fps: 38:\n",
      "Gif Salvo\n",
      "Episode 130# r: -20.0# Loss: 0.00148973 # Trains: 38201.75 # eps: 0.331# Space: 100.0% 'fps: 37:\n",
      "Episode 131# r: -20.0# Loss: 0.00152509 # Trains: 38525.25 # eps: 0.328# Space: 100.0% 'fps: 37:\n",
      "Episode 132# r: -21.0# Loss: 0.00161762 # Trains: 38799.75 # eps: 0.324# Space: 100.0% 'fps: 37:\n",
      "Episode 133# r: -21.0# Loss: 0.00140906 # Trains: 39065.5 # eps: 0.322# Space: 100.0% 'fps: 37:\n",
      "Episode 134# r: -21.0# Loss: 0.00120036 # Trains: 39340.25 # eps: 0.318# Space: 100.0% 'fps: 35:\n",
      "Episode 135# r: -19.0# Loss: 0.00140885 # Trains: 39672.0 # eps: 0.315# Space: 100.0% 'fps: 32:\n",
      "Episode 136# r: -21.0# Loss: 0.0013397 # Trains: 39967.75 # eps: 0.311# Space: 100.0% 'fps: 34:\n",
      "-------------------------UPDATED TARGET MODEL() ------------------------------\n",
      "Episode 137# r: -21.0# Loss: 0.00146936 # Trains: 40231.0 # eps: 0.309# Space: 100.0% 'fps: 36:\n",
      "Episode 138# r: -21.0# Loss: 0.00141629 # Trains: 40485.0 # eps: 0.306# Space: 100.0% 'fps: 37:\n",
      "Episode 139# r: -21.0# Loss: 0.00143806 # Trains: 40739.0 # eps: 0.303# Space: 100.0% 'fps: 35:\n",
      "Gif Salvo\n",
      "Episode 140# r: -21.0# Loss: 0.00151988 # Trains: 41014.25 # eps: 0.3# Space: 100.0% 'fps: 34:\n",
      "Episode 141# r: -21.0# Loss: 0.00144971 # Trains: 41276.75 # eps: 0.297# Space: 100.0% 'fps: 33:\n",
      "Episode 142# r: -21.0# Loss: 0.00163761 # Trains: 41529.5 # eps: 0.294# Space: 100.0% 'fps: 32:\n",
      "Episode 143# r: -20.0# Loss: 0.0016536 # Trains: 41819.25 # eps: 0.291# Space: 100.0% 'fps: 37:\n",
      "Episode 144# r: -21.0# Loss: 0.00140614 # Trains: 42080.75 # eps: 0.288# Space: 100.0% 'fps: 31:\n",
      "Episode 145# r: -21.0# Loss: 0.00157658 # Trains: 42364.75 # eps: 0.285# Space: 100.0% 'fps: 27:\n",
      "-------------------------UPDATED TARGET MODEL() ------------------------------\n",
      "Episode 146# r: -20.0# Loss: 0.00154897 # Trains: 42657.25 # eps: 0.282# Space: 100.0% 'fps: 35:\n",
      "Episode 147# r: -21.0# Loss: 0.00137696 # Trains: 42911.0 # eps: 0.279# Space: 100.0% 'fps: 36:\n",
      "Episode 148# r: -21.0# Loss: 0.00136689 # Trains: 43189.5 # eps: 0.276# Space: 100.0% 'fps: 28:\n",
      "Episode 149# r: -21.0# Loss: 0.00147241 # Trains: 43442.25 # eps: 0.273# Space: 100.0% 'fps: 22:\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    game_constructor = GameConstructor('Pong-v0')\n",
    "    game_constructor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04ace2d-0fde-4b3b-bda0-6c71c0b28948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f162935-ae49-4b11-b4ff-2a32974b36b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49193ead-816a-4457-a371-20694f49d8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f1bd30-4e50-4d08-87d2-1e02b4e72844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeb0a69-3cb1-4ab3-8737-9e1682cc2517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe1fdbf-0a0a-4019-8aa7-7e7ee579aca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e05ca25-454b-4080-a609-ad0be2169f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
