{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c66da-8f1a-4678-907c-223f24e6fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import imageio\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a9f3f2-5c94-42ea-a372-17589aa0562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "LEN_MEMORY_QUEUE = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac0b4e5-f68a-4337-a927-217a77c3007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def huber_loss(y_true, y_pred, clip_delta=1.0):\n",
    "    error = y_true - y_pred\n",
    "    cond  = tf.keras.backend.abs(error) < clip_delta\n",
    "\n",
    "    squared_loss = 0.5 * tf.keras.backend.square(error)\n",
    "    linear_loss  = clip_delta * (tf.keras.backend.abs(error) - 0.5 * clip_delta)\n",
    "\n",
    "    return tf.where(cond, squared_loss, linear_loss)\n",
    "\n",
    "def huber_loss_mean(y_true, y_pred, clip_delta=1.0):\n",
    "    return tf.keras.backend.mean(huber_loss(y_true, y_pred, clip_delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ab3c61-85e5-43ac-b754-f72abe9b5df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(): \n",
    "    def __init__(self, state_size, action_size):\n",
    "        #self.weight_backup              =       experiment_params['dirs']['dir_model']+experiment_params['game']+\".h5\"\n",
    "        self.state_size                 =       state_size\n",
    "        self.action_size                =       action_size\n",
    "        self.memory                     =       deque(maxlen=LEN_MEMORY_QUEUE)\n",
    "        self.min_learning_rate          =       .001\n",
    "        self.max_learning_rate          =       .0008\n",
    "        self.epochs_interval_lr         =       1\n",
    "        self.learning_rate_decay        =       (self.max_learning_rate - self.min_learning_rate) / self.epochs_interval_lr\n",
    "        self.gamma                      =       .99\n",
    "        self.exploration_rate           =       .7\n",
    "        self.exploration_min            =       .1\n",
    "        self.exploration_decay          =       1 / 90000 # A variável irá se atualizar.\n",
    "#        self.exploration_map            =       experiment_params['params_agent']['exploration_map']\n",
    "        self.k_frames                   =       4\n",
    "        self.frame_height               =       self.state_size[0]\n",
    "        self.frame_width                =       self.state_size[1]\n",
    "        self.brain                      =       self._build_model()\n",
    "        self.brain_target               =       self._build_model()\n",
    "        self.freq_update_nn             =       1000\n",
    "        self.initial_start_size         =       5000\n",
    "        \n",
    "        #self.update_exploration_decay(0)\n",
    "\n",
    "    # def update_exploration_decay(self, frame):\n",
    "    #     for frame_limit, eps_limit in self.exploration_map:\n",
    "    #         if frame <= frame_limit:\n",
    "    #             self.exploration_decay =  (eps_limit[0] - eps_limit[1]) / (frame_limit/self.k_frames)\n",
    "    #             break\n",
    "        \n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        \n",
    "        ## Build Convolutional NetWork\n",
    "        model.add(Conv2D(32, (8,8), strides=4, input_shape=(self.state_size[0], self.state_size[1], self.k_frames), activation='relu', padding='valid'))\n",
    "        model.add(Conv2D(64, (4,4), strides=2, activation='relu', padding='valid'))\n",
    "        model.add(Conv2D(64, (3,3), strides=1, activation='relu', padding='valid'))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        ## Build Neural Network\n",
    "        model.add(Dense(512, activation='relu', kernel_initializer='he_uniform'))          \n",
    "        #model.add(Dense(256, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "        #model.add(Dense(64, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "            \n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss=huber_loss_mean, optimizer=Adam(lr=self.max_learning_rate))\n",
    "            \n",
    "        model.summary()\n",
    "        # if self.has_load_data():\n",
    "        #     model.load_weights(self.weight_backup)\n",
    "        #     self.exploration_rate = self.exploration_min\n",
    "        #     self.max_learning_rate = self.min_learning_rate\n",
    "        return model\n",
    "\n",
    "    \n",
    "    # def save_model(self):\n",
    "    #     self.brain.save(self.weight_backup)\n",
    "        \n",
    "            \n",
    "    def get_last_k_frames(self, state):\n",
    "        frames = np.empty((self.k_frames, self.frame_height, self.frame_width))\n",
    "\n",
    "        for i in range(0, self.k_frames):\n",
    "            _, _, _, n_s, _ = self.memory[len(self.memory)-(self.k_frames-i)]\n",
    "            frames[i] = n_s\n",
    "            \n",
    "        return np.transpose(frames, axes=(1,2,0))\n",
    "\n",
    "    \n",
    "    def act(self, state, current):\n",
    "        if np.random.rand() <= self.exploration_rate or len(self.memory) < self.k_frames+1 or current < self.initial_start_size:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        k_frames_state = self.get_last_k_frames(state)\n",
    "        k_frames_state = np.expand_dims(k_frames_state, axis=0)\n",
    "        #act_values = self.brain.predict(k_frames_state, verbose=0)\n",
    "        act_values = self.brain.predict(k_frames_state, verbose=0)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):        \n",
    "        self.memory.append((state, action, reward, next_state, done))      \n",
    "\n",
    "\n",
    "    def pack_K_frames(self, sample_batch_size):\n",
    "        if len(self.memory) < self.k_frames+2:\n",
    "            return\n",
    "        \n",
    "        state = np.empty((sample_batch_size, self.k_frames, self.frame_height, self.frame_width))\n",
    "        action = np.empty(sample_batch_size, dtype=np.uint8)\n",
    "        reward = np.empty(sample_batch_size, dtype=np.float32)\n",
    "        next_state = np.empty((sample_batch_size, self.k_frames, self.frame_height, self.frame_width))\n",
    "        done = np.empty(sample_batch_size, dtype=np.bool_)\n",
    "        \n",
    "\n",
    "        for k in range(sample_batch_size):\n",
    "            index = random.randint(0, len(self.memory)-self.k_frames-2)\n",
    "            \n",
    "            for i, idx_memory in enumerate(range(index, index+self.k_frames)):\n",
    "                s, a, r, n_s, d = self.memory[idx_memory]\n",
    "            \n",
    "                state[k][i] = s\n",
    "                next_state[k][i] = n_s\n",
    "                \n",
    "            done[k] = d\n",
    "            action[k] = a\n",
    "            reward[k] = r\n",
    "              \n",
    "        #State = (32,4,84,84)  -> State Transpose = (32,84,84,4) \n",
    "        return np.transpose(state, axes=(0,2,3,1)), action, reward, np.transpose(next_state, axes=(0,2,3,1)), done\n",
    "        \n",
    "\n",
    "    def replay(self, sample_batch_size):\n",
    "        if len(self.memory) < sample_batch_size:\n",
    "            return\n",
    "        #sample_batch = random.sample(self.memory, sample_batch_size)\n",
    "        state, action, reward, next_state, done = self.pack_K_frames(sample_batch_size)\n",
    "        \n",
    "        #print('State: {}'.format(state.shape))\n",
    "        #print('Action: {}'.format(action.shape))\n",
    "        #print('Reward: {}'.format(reward.shape))\n",
    "        #print('Next_State: {}'.format(next_state.shape))\n",
    "        #print('Done: {}'.format(done.shape))\n",
    "        #input()\n",
    "        #target = reward\n",
    "        predicted = self.brain_target.predict(next_state, verbose=0) #Previsão proximo estado.\n",
    "        target_f = self.brain.predict(state, verbose=0) #Previsão estado atual.\n",
    "        #print('Predicted: {}'.format(predicted))\n",
    "        #print('Predicted[0]: {}'.format(predicted[2]))\n",
    "        #print('Predicted Max: {}'.format(np.amax(predicted)))\n",
    "        #print('Reward: {}'.format(reward))\n",
    "        #print('Target_f: {}'.format(target_f))\n",
    "        #input()\n",
    "        \n",
    "        # for i in range(sample_batch_size):\n",
    "        #     target = reward[i] + (self.gamma * np.amax(predicted[i]) * (1-done[i]))\n",
    "        #     target_f[i][action[i]] = target\n",
    "        targets = reward + (self.gamma * np.amax(predicted, axis=1) * (1 - done))\n",
    "        target_f[np.arange(sample_batch_size), action] = targets\n",
    "   \n",
    "            \n",
    "        \n",
    "        #print('Target_f Formatado: {}'.format(target_f))\n",
    "        #input()\n",
    "        history = self.brain.fit(state, target_f, batch_size=sample_batch_size, epochs=1, verbose=0)\n",
    "            \n",
    "        if self.exploration_rate > self.exploration_min:\n",
    "            self.exploration_rate -= self.exploration_decay\n",
    "        \n",
    "        return history, self.exploration_rate\n",
    "\n",
    "    def update_target_model(self, current_frame):\n",
    "        if (current_frame % self.freq_update_nn == 0 and TRAIN and current_frame > self.initial_start_size):\n",
    "            self.brain_target.set_weights(self.brain.get_weights())\n",
    "            print('-------------------------UPDATED TARGET MODEL() ------------------------------')\n",
    "\n",
    "    def update_learning_rate(self,):\n",
    "        ## Modified \n",
    "        if self.max_learning_rate > self.min_learning_rate:\n",
    "            if self.epochs_interval_lr > 0:\n",
    "                self.max_learning_rate-=self.learning_rate_decay\n",
    "                self.epochs_interval_lr-=1\n",
    "                K.set_value(self.brain.optimizer.lr, self.max_learning_rate) # Change learning rate.\n",
    "                print('Current Learning rate: {}'.format(K.eval(self.brain.optimizer.lr)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91f0054-4e0f-495a-9e22-f957c3ebda68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameConstructor():\n",
    "\n",
    "    def __init__(self, game):\n",
    "        self.env = gym.make(game)\n",
    "        self.sample_batch_size          =              32\n",
    "        self.episodes                   =              1000\n",
    "        self.action_size                =              self.env.action_space.n\n",
    "        self.state_size                 =              (84,84)\n",
    "        self.agent                      =              Agent(self.state_size, self.action_size)\n",
    "        self.best_score                 =              -99999999\n",
    "        self.crop_on_top                =              34\n",
    "        self.crop_on_bottom             =              16\n",
    "        self.crop_on_left               =              7\n",
    "        self.crop_on_right              =              7\n",
    "        self.frames_skip                =              1\n",
    "        self.current_frame              =              0\n",
    "        #self.freq_update                =              10000\n",
    "        #self.canSave                    =              True\n",
    "        self.time_train_init            =              time.time()\n",
    "#        self.epochs_to_save_results     =              experiment_params['epochs_to_save_results']\n",
    "        self.frames_in_atual_episode    =              0\n",
    "        self.seconds_in_atual_episode   =              0\n",
    "        self.freq_save_video            =              10\n",
    "\n",
    "        self.initialize_dirs()\n",
    "\n",
    "    def initialize_dirs(self):\n",
    "        for values in [\n",
    "            'experiments/Pong-useToPaper01/movies/'\n",
    "        ]:\n",
    "            if not os.path.isdir(values):\n",
    "                os.makedirs(values)\n",
    "    \n",
    "    def to_gray_scale(self, img):\n",
    "        return 0.299*img[:,:,0] + 0.587*img[:,:,1] + 0.114*img[:,:,2]\n",
    "    \n",
    "    def get_frames_per_seconds_in_atual_episode(self):\n",
    "        return int(self.frames_in_atual_episode / (time.time() - self.seconds_in_atual_episode))\n",
    "    \n",
    "   \n",
    "    def crop_img(self, img):\n",
    "        #return img[self.crop_on_top: -self.crop_on_bottom, self.crop_on_border:-self.crop_on_border]\n",
    "        return img[self.crop_on_top:-self.crop_on_bottom, self.crop_on_left:-self.crop_on_right]    \n",
    "\n",
    "    def preprocess_img(self, img):\n",
    "        return self.to_gray_scale(cv2.resize(self.crop_img(img), (self.state_size[0],self.state_size[0]), interpolation=cv2.INTER_AREA)) / 255\n",
    "        \n",
    "    \n",
    "    def save_image_epoch(self, gif_to_save, epoch, best_current_play=False):\n",
    "        if best_current_play:\n",
    "            epoch = str(epoch)+'_best'\n",
    "        else:\n",
    "            epoch = str(epoch)\n",
    "        \n",
    "        #if (self.canSave and len(gif_to_save) > 0):\n",
    "        if (len(gif_to_save) > 0):\n",
    "            dir_save = 'experiments/Pong-useToPaper01/movies/'\n",
    "            imageio.mimwrite(dir_save+epoch+'.mp4', np.multiply(gif_to_save, 255).astype(np.uint8), fps = 100)\n",
    "            print('Gif Salvo') \n",
    "            #self.canSave = False\n",
    "            \n",
    "    def restart_chronometer(self):\n",
    "        self.frames_in_atual_episode=0\n",
    "        self.seconds_in_atual_episode=0\n",
    "    \n",
    "    def run(self):\n",
    "        #global total_reward_game\n",
    "\n",
    "        try:\n",
    "            for i_episodes in range(self.episodes):\n",
    "                state = self.env.reset()\n",
    "               \n",
    "                state = self.preprocess_img(state)\n",
    "                \n",
    "                done = False\n",
    "                current_images_episode = [] \n",
    "                total_reward=0\n",
    "                history_list = []\n",
    "                exploration = 1.0\n",
    "                self.seconds_in_atual_episode= time.time()\n",
    "                \n",
    "                while not done:\n",
    "                                        \n",
    "                    #if i_episodes > 600 or not TRAIN:\n",
    "                    #self.env.render()\n",
    "                    action = self.agent.act(state, self.current_frame)\n",
    "                    next_state, reward, done, info = self.env.step(action)\n",
    "                    \n",
    "                    reward = np.sign(reward)\n",
    "                    total_reward+=reward\n",
    "                    next_state = self.preprocess_img(next_state)\n",
    "                    \n",
    "                    current_images_episode.append(next_state)\n",
    "\n",
    "                    \n",
    "                    self.agent.remember(state, action, reward, next_state, done)\n",
    "                    state = next_state\n",
    "                        \n",
    "                    self.current_frame+=1\n",
    "                    self.frames_in_atual_episode+=1\n",
    "                    if TRAIN:\n",
    "                        if self.current_frame > self.agent.initial_start_size and (self.current_frame % self.frames_skip) == 0:\n",
    "                           \n",
    "                            history, exploration = self.agent.replay(self.sample_batch_size)\n",
    "                            history_list.append(history.history['loss'])\n",
    "                    \n",
    "                        self.agent.update_target_model(self.current_frame)\n",
    "    \n",
    "                #total_reward_game.append(total_reward)\n",
    "                \n",
    "                \n",
    "                if TRAIN and self.current_frame > self.agent.initial_start_size:\n",
    "                    self.agent.update_learning_rate()\n",
    "                    #self.agent.update_exploration_decay(self.current_frame)\n",
    "                    if i_episodes % self.freq_save_video == 0:\n",
    "                        #self.canSave=True\n",
    "                        \n",
    "                        self.save_image_epoch( current_images_episode, i_episodes)   \n",
    "                    \n",
    "                    # self.add_new_result(total_reward, (self.current_frame / self.frames_skip), time.time(), \n",
    "                    #                     np.mean(history_list), exploration, self.get_frames_per_seconds_in_atual_episode(), K.eval(breakout.agent.brain.optimizer.lr))\n",
    "                   \n",
    "                    # if total_reward > self.best_score and self.current_frame > self.agent.initial_start_size:\n",
    "                    #         self.best_score = total_reward\n",
    "                    #         self.agent.save_model()\n",
    "                    #         self.save_image_epoch(current_images_episode, i_episodes, best_current_play=True)\n",
    "                    #         print('Save best current model -> ', end='')\n",
    "                current_images_episode = []\n",
    "                print(\"Episode {}# r: {}# Loss: {:.6} # Trains: {} # eps: {:.3}# Space: {}% 'fps: {}:\".format(i_episodes, total_reward, np.mean(history_list), (self.current_frame / self.frames_skip), exploration, round(((len(self.agent.memory) / (LEN_MEMORY_QUEUE)) * 100), 2), self.get_frames_per_seconds_in_atual_episode() ))\n",
    "                self.restart_chronometer()\n",
    "                \n",
    "                \n",
    "        finally:\n",
    "            if TRAIN:\n",
    "                print('Finish')\n",
    "                #self.agent.save_model()\n",
    "                #self.save_results()\n",
    "                \n",
    "                \n",
    "            self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c105c47-5df7-4b5d-a031-e305ca30a00e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    game_constructor = GameConstructor('Pong-v0')\n",
    "    game_constructor.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
